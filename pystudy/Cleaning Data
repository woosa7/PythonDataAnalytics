-----------------------------------------------------------
Cleaning Data - melt, pivot, concat, merge
-----------------------------------------------------------

-----------------------------------------------------------
## 2. Tidying data
-----------------------------------------------------------

-----------------------------------------------------------
3. Combining data
-----------------------------------------------------------
Combining rows of data


# row_concat
row_concat = pd.concat([uber1, uber2, uber3])

print(row_concat.shape)
print(row_concat.head())

-----------------------------------------------------------
Combining columns of data


# column-wise concatenation
ebola_tidy = pd.concat([ebola_melt, status_country], axis=1)

print(ebola_tidy.shape)
print(ebola_tidy.head())

-----------------------------------------------------------
Concat many files - Finding files that match a pattern

import glob
import pandas as pd

# matched file list
pattern = '*.csv'
csv_files = glob.glob(pattern)
print(csv_files)

csv2 = pd.read_csv(csv_files[1])
print(csv2.head())


#  Iterate over csv_files

frames = []

for csv in csv_files:
    df = pd.read_csv(csv)
    frames.append(df)

# Concatenate frames into a single DataFrame: uber
uber = pd.concat(frames)

print(uber.shape)
print(uber.head())

-----------------------------------------------------------
Merge
1-to-1 data merge


# Merge the DataFrames: o2o
o2o = pd.merge(left=site, right=visited, left_on='name', right_on='site')
print(o2o)

-----------------------------------------------------------
Many-to-1 data merge


# Merge the DataFrames: m2o
m2o = pd.merge(left=site, right=visited, left_on='name', right_on='site')

# Print m2o
print(m2o)

-----------------------------------------------------------
Many-to-many data merge : do not have unique keys for a merge


# Merge site and visited: m2m
m2m = pd.merge(left=site, right=visited, left_on='name', right_on='site')

# Merge m2m and survey: m2m
m2m = pd.merge(left=m2m, right=survey, left_on='ident', right_on='taken')

# Print the first 20 lines of m2m
print(m2m.head(20))



-----------------------------------------------------------
4. Converting data types
-----------------------------------------------------------

# Convert to 'category' type
tips.sex = tips.sex.astype('category')
tips.smoker = tips.smoker.astype('category')

print(tips.info())

# Convert to a numeric dtype
# 'coerce' : error --> a missing value, NaN

tips['total_bill'] = pd.to_numeric(tips['total_bill'], errors='coerce')
tips['tip'] = pd.to_numeric(tips['tip'], errors='coerce')

print(tips.info())

-----------------------------------------------------------
String parsing with regular expressions


import re

# Compile the pattern: phone number
prog = re.compile('\d{3}-\d{3}-\d{4}')

# See if the pattern matches
result = prog.match('123-456-7890')
print(bool(result))

# See if the pattern matches
result = prog.match('1123-456-7890')
print(bool(result))

-----------------------------------------------------------
Extracting numerical values from strings


import re

# Find the numeric values: matches
matches = re.findall('\d+', 'the recipe calls for 10 strawberries and 1 banana')
print(matches)

-----------------------------------------------------------
Pattern matching


print(bool(re.match(pattern='\d{3}-\d{3}-\d{4}', string='123-456-7890')))

print(bool(re.match(pattern='\$\d*\.\d{2}', string='$123.45')))

print(bool(re.match(pattern='[A-Z]\w*', string='Australia')))

-----------------------------------------------------------
Custom functions to clean data


# Female to 0, Male to 1
def recode_gender(gender):

    if gender == 'Male':
        return 1
    elif gender == 'Female':
        return 0
    else:
        return np.nan

# Apply the function to the sex column
tips['recode'] = tips.sex.apply(recode_gender)
print(tips.head())


-----------------------------------------------------------
Lambda functions


# lambda function using replace
tips['total_dollar_replace'] = tips.total_dollar.apply(lambda x: x.replace('$', ''))

# lambda function using regular expressions
tips['total_dollar_re'] = tips.total_dollar.apply(lambda x: re.findall('\d+\.\d+', x))

print(tips.head())

-----------------------------------------------------------
Drop duplicate data


# Create the new DataFrame: tracks
tracks = billboard[['year', 'artist', 'track', 'time']]
print(tracks.info())

# Drop the duplicates
tracks_no_duplicates = tracks.drop_duplicates()
print(tracks_no_duplicates.info())


-----------------------------------------------------------
Fill missing data


# Calculate the mean of the Ozone column
oz_mean = airquality.Ozone.mean()

# Replace all the missing values in the Ozone column with the mean
airquality['Ozone'] = airquality.Ozone.fillna(oz_mean)

print(airquality.info())

-----------------------------------------------------------
Testing your data with asserts


# Assert that there are no missing values
assert pd.notnull(ebola).all().all()

# Assert that all values are >= 0
assert (ebola >= 0).all().all()




-----------------------------------------------------------
5. Case study
-----------------------------------------------------------
Visualizing your data


import matplotlib.pyplot as plt

# Create the scatter plot
g1800s.plot(kind='scatter', x='1800', y='1899')

plt.xlabel('Life Expectancy in 1800')
plt.ylabel('Life Expectancy in 1899')
plt.xlim(20, 55)
plt.ylim(20, 55)

plt.show()

-----------------------------------------------------------
Check column values


def check_null_or_valid(row_data):
    """Function that takes a row a data,
    drops all missing values,
    and checks if all remaining values are greater than or equal to 0
    """
    no_na = row_data.dropna()[1:-1]
    numeric = pd.to_numeric(no_na)
    ge0 = numeric >= 0
    return ge0

# Check whether the first column is 'Life expectancy'
assert g1800s.columns[0] == 'Life expectancy'

# Check whether the values in the row are valid
assert g1800s.iloc[:, 1:].apply(check_null_or_valid, axis=1).all().all()

# Check that there is only one instance of each country
assert g1800s['Life expectancy'].value_counts()[0] == 1

-----------------------------------------------------------
Assembling your data


# Concatenate the DataFrames row-wise
gapminder = pd.concat([g1800s, g1900s, g2000s])

# Print the shape of gapminder
print(gapminder.shape)

# Print the head of gapminder
print(gapminder.head())

-----------------------------------------------------------
Reshaping your data


# Melt
# separate column for each year -->
# a single column for the year, and a single column that represents the average life expectancy for each year and country
gapminder_melt = pd.melt(gapminder, id_vars='Life expectancy')

# Rename the columns
gapminder_melt.columns = ['country', 'year', 'life_expectancy']

print(gapminder_melt.head())

-----------------------------------------------------------
Checking the data types


# Convert the year column to numeric
gapminder.year = pd.to_numeric(gapminder.year)

# Test if country is of type object
assert gapminder.country.dtypes == np.object

# Test if year is of type int64
assert gapminder.year.dtypes == np.int64

# Test if life_expectancy is of type float64
assert gapminder.life_expectancy.dtypes == np.float64

-----------------------------------------------------------
Looking at country spellings

countries = gapminder['country']

# Drop all the duplicates from countries
countries = countries.drop_duplicates()

# Write the regular expression: pattern
pattern = '^[A-Za-z\.\s]*$'

# Create the Boolean vector: mask
mask = countries.str.contains(pattern)

# Invert the mask
mask_inverse = ~mask

# Subset countries using mask_inverse
invalid_countries = countries.loc[mask_inverse]

print(invalid_countries)

-----------------------------------------------------------
More data cleaning and processing

* In general, it is not the best idea to drop missing values, because in doing so you may end up throwing away useful information.

# Assert that country does not contain any missing values
assert pd.notnull(gapminder.country).all()

# Assert that year does not contain any missing values
assert pd.notnull(gapminder.year).all()

# Drop the missing values
gapminder = gapminder.dropna(how='any')

# Print the shape of gapminder
print(gapminder.shape)

-----------------------------------------------------------
Wrapping up


plt.subplot(2, 1, 1)

# Create a histogram of life_expectancy
gapminder.life_expectancy.plot(kind='hist')

# Group gapminder: gapminder_agg
gapminder_agg = gapminder.groupby('year')['life_expectancy'].mean()

# Print the head of gapminder_agg
print(gapminder_agg.head())

# Print the tail of gapminder_agg
print(gapminder_agg.tail())

# Add second subplot
plt.subplot(2, 1, 2)

# Create a line plot of life expectancy per year
gapminder_agg.plot()

# Add title and specify axis labels
plt.title('Life expectancy over the years')
plt.ylabel('Life expectancy')
plt.xlabel('Year')

# Display the plots
plt.tight_layout()
plt.show()

# Save both DataFrames to csv files
gapminder.to_csv('gapminder.csv')
gapminder_agg.to_csv('gapminder_agg.csv')

-----------------------------------------------------------
END
-----------------------------------------------------------